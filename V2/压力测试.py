import random
import time
from faker import Faker
from datetime import datetime, timedelta
import json
import os
import threading
from concurrent.futures import ThreadPoolExecutor
from natsort import natsort_keygen

# --- é…ç½®å‚æ•° ---
NUM_BOOKS = 1000000  # æƒ³è¦ç”Ÿæˆçš„å›¾ä¹¦æ¯è®°å½•æ•°é‡
MAX_RECORDS_PER_FILE = 999  # æ¯ä¸ª JSON æ–‡ä»¶æœ€å¤§è®°å½•æ•°
NUM_THREADS = 16  # ä½¿ç”¨çš„çº¿ç¨‹æ•°é‡

# !!! ä½ çš„è‡ªå®šä¹‰è·¯å¾„ (ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸² r"..." ä¿æŒä¸å˜) !!!
MOTHER_DATA_DIR = r"E:\pyide_only\å­¦ä¹ \å›¾ä¹¦ç®¡ç†ç³»ç»Ÿ\db\data"
COPY_DATA_DIR = r"E:\pyide_only\å­¦ä¹ \å›¾ä¹¦ç®¡ç†ç³»ç»Ÿ\db\data-b"

# éšæœºæ•°æ®é…ç½® (ä¿æŒä¸å˜)
CATEGORIES = ["å°è¯´", "ç§‘æŠ€", "å†å²", "ä¼ è®°", "è‰ºæœ¯", "æ•™è‚²", "è®¡ç®—æœº"]
STATUSES = ["æ­£å¸¸", "æ­£å¸¸", "æ­£å¸¸", "å€Ÿå‡º", "å€Ÿå‡º", "å€Ÿå‡º", "ä¸¢å¤±", "æŸå"]
PUBLISHERS = ["äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾", "ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾", "æœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾", "æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾", "æµ‹è¯•å‡ºç‰ˆç¤¾"]
MIN_QUANTITY = 1
MAX_QUANTITY = 5

# --- å…¨å±€åŒæ­¥æœºåˆ¶ ---
global_copy_count = 0

# ä¸ºæ¯æœ¬æ–‡ä»¶å†™å…¥åˆ›å»ºä¸€ä¸ªå…¨å±€é”ï¼Œç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªçº¿ç¨‹åœ¨è¿›è¡Œæ¯æœ¬ I/O
mother_file_io_lock = threading.Lock()


# --- è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ---

def generate_random_isbn():
    group_prefix = "978-7"
    publisher_code = f"{random.randint(10000, 99999):05d}"
    title_code = f"{random.randint(10, 99):02d}"
    check_digit = random.randint(0, 9)
    return f"{group_prefix}-{publisher_code}-{title_code}-{check_digit}"


def generate_random_datetime(faker_instance, start_days_ago=365):
    """å‡½æ•°æ¥å—ä¸€ä¸ª Faker å®ä¾‹"""
    end_date = datetime.now()
    start_date = end_date - timedelta(days=start_days_ago)
    random_date = faker_instance.date_time_between(start_date=start_date, end_date=end_date)
    return random_date.strftime("%Y-%m-%d %H:%M:%S")


# --- æ–‡ä»¶åŠ è½½å’Œä¿å­˜ (å·²ä¼˜åŒ– _save_data) ---

def _load_data(directory, filename):
    """å®‰å…¨åœ°åŠ è½½ JSON æ–‡ä»¶"""
    file_path = os.path.join(directory, filename)
    if not os.path.exists(file_path):
        return {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            return json.loads(content) if content else {}
    except json.JSONDecodeError:
        return {}
    except Exception:
        return {}


def _save_data(directory, filename, data):
    """å®‰å…¨åœ°ä¿å­˜ JSON æ–‡ä»¶ï¼Œç§»é™¤å†…éƒ¨é”é€»è¾‘ï¼Œç”±è°ƒç”¨æ–¹è´Ÿè´£åŠ é”"""
    file_path = os.path.join(directory, filename)
    try:
        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰é”æ“ä½œï¼Œç”±è°ƒç”¨æ–¹è´Ÿè´£åŠ é”
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"å†™å…¥æ–‡ä»¶ {filename} å¤±è´¥: {e}")


# --- æ ¸å¿ƒæ•°æ®ç”Ÿæˆå‡½æ•° (ä¿æŒä¸å˜) ---

def generate_single_book_record(mother_id, fake_instance):
    """ç”Ÿæˆå•ä¸ªå›¾ä¹¦æ¯è®°å½•åŠå…¶æ‰€æœ‰å‰¯æœ¬æ•°æ®çš„å­—å…¸"""

    # ä½¿ç”¨çº¿ç¨‹éš”ç¦»çš„ fake_instance æ›¿ä»£å…¨å±€ fake
    title = fake_instance.catch_phrase() + " - " + fake_instance.last_name() + "ä¼ "
    quantity = random.randint(MIN_QUANTITY, MAX_QUANTITY)
    date_added = generate_random_datetime(fake_instance)

    book_record = {
        "mother_id": mother_id,
        "name": title,
        "author": fake_instance.name(),
        "publisher": random.choice(PUBLISHERS),
        "isbn": generate_random_isbn(),
        "pages": str(random.randint(100, 800)),
        "words": str(random.randint(5, 100)) + "ä¸‡å­—",
        "category": random.choice(CATEGORIES),
        "date_added": date_added,
        "copies_list": []
    }

    for i in range(1, quantity + 1):
        copy_id = f"{mother_id}-{i}"
        status = random.choice(STATUSES)

        borrower_name = None
        borrow_date = None
        due_date = None
        notes = None

        if status == "å€Ÿå‡º":
            borrower_name = fake_instance.name()
            start_date_obj = datetime.strptime(date_added, "%Y-%m-%d %H:%M:%S")
            borrow_date_obj = fake_instance.date_time_between(start_date=start_date_obj, end_date=datetime.now())

            borrow_date = borrow_date_obj.strftime("%Y-%m-%d %H:%M:%S")
            due_date_obj = borrow_date_obj + timedelta(days=random.randint(7, 30))
            due_date = due_date_obj.strftime("%Y-%m-%d %H:%M:%S")
            notes = fake_instance.sentence(nb_words=3) if random.random() < 0.2 else None

        copy_record = {
            "copy_id": copy_id,
            "book_id": mother_id,
            "status": status,
            "borrower_name": borrower_name,
            "borrow_date": borrow_date,
            "due_date": due_date,
            "notes": notes
        }

        book_record["copies_list"].append(copy_record)

    return book_record


# --- çº¿ç¨‹ä»»åŠ¡æ‰§è¡Œå‡½æ•° (ä¿®å¤æ¯æœ¬å¹¶å‘å†™å…¥é—®é¢˜) ---

def generate_and_save_task(mother_index_start, mother_index_end):
    """
    è´Ÿè´£ç”Ÿæˆå¹¶å®‰å…¨ä¿å­˜æŒ‡å®šèŒƒå›´å†…çš„æ¯æœ¬æ•°æ®ï¼Œå¹¶è¿”å›å‰¯æœ¬æ•°æ®åˆ—è¡¨ã€‚
    """
    thread_name = threading.current_thread().name
    print(f"{thread_name}: ä»»åŠ¡èŒƒå›´ {mother_index_start} åˆ° {mother_index_end}")

    local_fake = Faker('zh_CN')

    thread_copy_records_list = []
    mother_file_buffer = {}

    for i in range(mother_index_start, mother_index_end):

        # 1. é¢„è®¡ç®— ID å’Œæ–‡ä»¶å
        mother_count_in_file = (i - 1) % MAX_RECORDS_PER_FILE + 1
        mother_file_index = (i - 1) // MAX_RECORDS_PER_FILE + 1
        mother_id = f"1-{mother_file_index}-{mother_count_in_file:03d}"
        mother_filename = f"book-{mother_file_index}.json"

        # 2. ç”Ÿæˆæ•°æ®
        book_data = generate_single_book_record(mother_id, local_fake)

        # 3. å‡†å¤‡æ¯æœ¬æ•°æ®æ ¼å¼
        mother_record = {
            "name": book_data["name"],
            "author": book_data["author"],
            "publisher": book_data["publisher"],
            "isbn": book_data["isbn"],
            "pages": book_data["pages"],
            "words": book_data["words"],
            "category": book_data["category"],
            "date_added": book_data["date_added"],
            "copies": [copy["copy_id"] for copy in book_data["copies_list"]]
        }

        # 4. å­˜å…¥æ¯æœ¬ç¼“å­˜
        if mother_filename not in mother_file_buffer:
            mother_file_buffer[mother_filename] = {}
        mother_file_buffer[mother_filename][mother_id] = mother_record

        # 5. æš‚å­˜å‰¯æœ¬æ•°æ®
        thread_copy_records_list.extend(book_data["copies_list"])

        if i % 10000 == 0:
            print(f"{thread_name}: å·²ç”Ÿæˆ {i - mother_index_start + 1} æ¡æ•°æ®...")

    # --- çº¿ç¨‹ I/O é˜¶æ®µ (å…³é”®ä¿®å¤åŒºåŸŸ) ---

    # å†™å…¥æ¯æœ¬æ–‡ä»¶ï¼šæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹è´Ÿè´£å†™å…¥
    for filename, records in mother_file_buffer.items():

        # ğŸš¨ å…³é”®ä¿®å¤ï¼šåŠ é”ï¼Œå°†è¯»å–ã€åˆå¹¶ã€å†™å…¥å˜æˆåŸå­æ“ä½œ
        mother_file_io_lock.acquire()

        try:
            # 1. åœ¨é”å†…å®‰å…¨åœ°è¯»å–æ–‡ä»¶
            existing_data = _load_data(MOTHER_DATA_DIR, filename)

            # 2. åœ¨é”å†…å®‰å…¨åœ°åˆå¹¶æ•°æ®
            existing_data.update(records)

            # 3. ğŸ†• æ–°å¢ï¼šå†™å…¥å‰å¯¹æ•°æ®è¿›è¡Œæ’åº (ä¿è¯æ–‡ä»¶å†…éƒ¨è®°å½•æœ‰åº)
            sorted_keys = sorted(existing_data.keys())
            sorted_existing_data = {k: existing_data[k] for k in sorted_keys}

            # 4. åœ¨é”å†…å®‰å…¨åœ°å†™å…¥æ•°æ®
            _save_data(MOTHER_DATA_DIR, filename, sorted_existing_data)

        except Exception as e:
            print(f"çº¿ç¨‹ {thread_name} å†™å…¥æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        finally:
            mother_file_io_lock.release()  # ç¡®ä¿é‡Šæ”¾é”

    print(f"{thread_name}: æ¯æœ¬å†™å…¥å®Œæˆã€‚")

    # è¿”å›çº¿ç¨‹ç”Ÿæˆçš„å‰¯æœ¬è®°å½•åˆ—è¡¨
    return thread_copy_records_list


# --- ä¸»æ‰§è¡Œå‡½æ•° (é›†ä¸­æ’åºä¸é¡ºåºå†™å…¥å‰¯æœ¬) ---

def run_multithreaded_generation():
    """è®¾ç½®ç›®å½•å¹¶åˆ†é…å¤šçº¿ç¨‹ä»»åŠ¡ï¼Œå¹¶åœ¨ä¸»çº¿ç¨‹ä¸­é›†ä¸­æ’åºå†™å…¥å‰¯æœ¬ã€‚"""
    print(f"--- å‡†å¤‡ç”Ÿæˆ {NUM_BOOKS} æ¡æ¯æœ¬è®°å½• ({NUM_THREADS} çº¿ç¨‹) ---")

    # 1. åˆ›å»ºç›®å½•
    os.makedirs(MOTHER_DATA_DIR, exist_ok=True)
    os.makedirs(COPY_DATA_DIR, exist_ok=True)

    # 2. åˆ†é…ä»»åŠ¡å’Œæ”¶é›†æ•°æ®
    futures = []
    chunk_size = NUM_BOOKS // NUM_THREADS
    all_copies_list = []  # æ”¶é›†æ‰€æœ‰çº¿ç¨‹çš„å‰¯æœ¬æ•°æ®

    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        for i in range(NUM_THREADS):
            start = i * chunk_size + 1
            end = (i + 1) * chunk_size + 1 if i < NUM_THREADS - 1 else NUM_BOOKS + 1

            future = executor.submit(generate_and_save_task, start, end)
            futures.append(future)

        # æ”¶é›†æ‰€æœ‰çº¿ç¨‹è¿”å›çš„å‰¯æœ¬æ•°æ®
        for future in futures:
            thread_copies = future.result()
            all_copies_list.extend(thread_copies)

    # 3. é›†ä¸­æ’åºå‰¯æœ¬æ•°æ®
    print("\n--- æ­£åœ¨æ’åºæ‰€æœ‰å‰¯æœ¬æ•°æ®... ---")
    # ğŸ†• å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ natsort_keygen() ä½œä¸º key
    # natsort_keygen() åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒèƒ½å°†å­—ç¬¦ä¸²åˆ†å‰²æˆæ•°å­—å’Œæ–‡æœ¬æ®µè¿›è¡Œæ•°å€¼æ¯”è¾ƒã€‚
    nat_key = natsort_keygen()
    all_copies_list.sort(key=lambda x: nat_key(x['copy_id']))

    print("--- æ’åºå®Œæˆï¼Œå¼€å§‹é¡ºåºå†™å…¥å‰¯æœ¬æ–‡ä»¶... ---")

    # 4. é¡ºåºå†™å…¥å‰¯æœ¬æ–‡ä»¶
    total_copies = len(all_copies_list)

    current_file_index = 1
    current_records = {}  # å½“å‰æ–‡ä»¶çš„æ•°æ®ç¼“å­˜

    for i, copy_record in enumerate(all_copies_list):
        copy_id = copy_record['copy_id']

        # å‡†å¤‡å†™å…¥ JSON çš„æ ¼å¼
        record_to_save = {
            "book_id": copy_record["book_id"],
            "status": copy_record["status"],
            "borrower_name": copy_record.get("borrower_name"),
            "borrow_date": copy_record.get("borrow_date"),
            "due_date": copy_record.get("due_date"),
            "notes": copy_record.get("notes")
        }
        current_records[copy_id] = record_to_save

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ–‡ä»¶ä¸Šé™
        if (i + 1) % MAX_RECORDS_PER_FILE == 0:
            filename = f"book-b-{current_file_index}.json"
            # å‰¯æœ¬å†™å…¥æ˜¯ä¸»çº¿ç¨‹é¡ºåºæ‰§è¡Œï¼Œæ— éœ€åŠ é”
            _save_data(COPY_DATA_DIR, filename, current_records)
            print(f"âœ… å†™å…¥å‰¯æœ¬æ–‡ä»¶: {filename}")

            # é‡ç½®ç¼“å­˜å’Œæ–‡ä»¶ç´¢å¼•
            current_records = {}
            current_file_index += 1

    # 5. å†™å…¥æœ€åä¸€ä¸ªæ–‡ä»¶ (å¦‚æœè¿˜æœ‰å‰©ä½™æ•°æ®)
    if current_records:
        filename = f"book-b-{current_file_index}.json"
        _save_data(COPY_DATA_DIR, filename, current_records)
        print(f"âœ… å†™å…¥å‰¯æœ¬æ–‡ä»¶: {filename} (å‰©ä½™)")

    # 6. ç»Ÿè®¡ç»“æœ
    global global_copy_count
    global_copy_count = total_copies

    total_copy_files = current_file_index if total_copies > 0 else 0

    print("\n==============================================")
    print("âœ… å…¨éƒ¨æ•°æ®ç”Ÿæˆå’Œä¿å­˜å®Œæ¯•ï¼")
    print(f"æ€»æ¯æœ¬è®°å½•æ•°: {NUM_BOOKS}")
    print(f"æ€»å‰¯æœ¬è®°å½•æ•°: {total_copies}")
    print(f"æ¯æœ¬æ–‡ä»¶æ•°é‡: {(NUM_BOOKS - 1) // MAX_RECORDS_PER_FILE + 1} ä¸ª (ä¿å­˜åœ¨ {MOTHER_DATA_DIR})")
    print(f"å‰¯æœ¬æ–‡ä»¶æ•°é‡: {total_copy_files} ä¸ª (ä¿å­˜åœ¨ {COPY_DATA_DIR})")
    print("==============================================")


# --- æ‰§è¡Œè„šæœ¬ ---
if __name__ == "__main__":
    start_time = time.time()
    run_multithreaded_generation()
    end_time = time.time()
    print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")